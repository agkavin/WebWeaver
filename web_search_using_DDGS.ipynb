{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "\n",
    "def get_data_from_website(url):\n",
    "    # Get response from the server\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 500:\n",
    "        print(\"Server error\")\n",
    "        return\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Removing js and css code\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # Extract text in markdown format\n",
    "    html = str(soup)\n",
    "    html2text_instance = html2text.HTML2Text()\n",
    "    html2text_instance.images_to_alt = True\n",
    "    html2text_instance.body_width = 0\n",
    "    html2text_instance.single_line_break = True\n",
    "    text = html2text_instance.handle(html)\n",
    "\n",
    "    # Extract page metadata\n",
    "    try:\n",
    "        page_title = soup.title.string.strip()\n",
    "    except:\n",
    "        page_title = url.path[1:].replace(\"/\", \"-\")\n",
    "    meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    meta_keywords = soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "    if meta_description:\n",
    "        description = meta_description.get(\"content\")\n",
    "    else:\n",
    "        description = page_title\n",
    "    if meta_keywords:\n",
    "        meta_keywords = meta_description.get(\"content\")\n",
    "    else:\n",
    "        meta_keywords = \"\"\n",
    "\n",
    "    metadata = {'title': page_title,\n",
    "                'url': url,\n",
    "                'description': description,\n",
    "                'keywords': meta_keywords}\n",
    "\n",
    "    return text, metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# Data Cleaning functions\n",
    "\n",
    "def merge_hyphenated_words(text):\n",
    "    return re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
    "\n",
    "\n",
    "def fix_newlines(text):\n",
    "    return re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_multiple_newlines(text):\n",
    "    return re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaning_functions = [merge_hyphenated_words, fix_newlines, remove_multiple_newlines]\n",
    "    for cleaning_function in cleaning_functions:\n",
    "        text = cleaning_function(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text to docs , then chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_docs(text, metadata):\n",
    "    doc_chunks = []\n",
    "    text_splitter = MarkdownTextSplitter(chunk_size=2048, chunk_overlap=128)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        doc_chunks.append(doc)\n",
    "    return doc_chunks\n",
    "\n",
    "\n",
    "def get_doc_chunks(text, metadata):\n",
    "    text = clean_text(text)\n",
    "    doc_chunks = text_to_docs(text, metadata)\n",
    "    return doc_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma DB client and vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "class ChromaClient:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(ChromaClient, cls).__new__(cls)\n",
    "            cls._instance._initialize()\n",
    "        return cls._instance\n",
    "\n",
    "    def _initialize(self):\n",
    "        embedding_function = OllamaEmbeddings(model=\"llama3.1:latest\")\n",
    "        self.client = Chroma(\n",
    "            collection_name=\"website_data\",\n",
    "            embedding_function=embedding_function,\n",
    "            persist_directory=\"data/chroma\"\n",
    "        )\n",
    "\n",
    "def get_chroma_client():\n",
    "    return ChromaClient().client\n",
    "\n",
    "def store_docs(url):\n",
    "    try:\n",
    "        text, metadata = get_data_from_website(url)\n",
    "        docs = get_doc_chunks(text, metadata)\n",
    "        \n",
    "        # Ensure the persist directory is writable\n",
    "        persist_directory = \"data/chroma\"\n",
    "        if not os.access(persist_directory, os.W_OK):\n",
    "            raise PermissionError(f\"Cannot write to directory: {persist_directory}\")\n",
    "\n",
    "        vector_store = get_chroma_client()\n",
    "\n",
    "        # Reset the collection\n",
    "        #if vector_store.collection_exists(\"website_data\"):\n",
    "        #    vector_store.delete_collection()\n",
    "\n",
    "        vector_store.add_documents(docs)\n",
    "        vector_store.persist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing documents: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "#system prompt template\n",
    "system_prompt_template = \"\"\"\n",
    "You are a knowledgeable support agent.\n",
    "\n",
    "Your role is to assist users by answering their queries based on the information provided from a specific website. Ensure your responses are accurate, informative, and derived directly from the given context. Avoid making up information or providing incorrect details.\n",
    "\n",
    "Use the context below to answer the user's question. If the context doesn't fully address the query, acknowledge the limitation and guide the user accordingly.\n",
    "\n",
    "----------------\n",
    "{context}\n",
    "{chat_history}\n",
    "Follow-up question:\n",
    "\"\"\"\n",
    "\n",
    "# generalized human prompt template\n",
    "human_prompt_template = \"{question}\\nAnswer:\"\n",
    "\n",
    "\n",
    "def get_prompt():\n",
    "    \n",
    "    # Create SystemMessagePromptTemplate with generalized system prompt\n",
    "    system_prompt = SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=['context', 'chat_history'],\n",
    "            template=system_prompt_template,\n",
    "            template_format='f-string',\n",
    "            validate_template=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create HumanMessagePromptTemplate with generalized human prompt\n",
    "    human_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=['question'],\n",
    "            template=human_prompt_template,\n",
    "            template_format='f-string',\n",
    "            validate_template=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return ChatPromptTemplate\n",
    "    return ChatPromptTemplate(\n",
    "        input_variables=['context', 'question', 'chat_history'],\n",
    "        messages=[system_prompt, human_prompt]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain and Response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain():\n",
    "    model = ChatOllama(model=\"llama3.1:latest\")\n",
    "    vector_store = get_chroma_client()\n",
    "    prompt = get_prompt()\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type=\"mmr\", verbose=True)\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        model,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=False,\n",
    "        combine_docs_chain_kwargs=dict(prompt=prompt),\n",
    "        verbose=False,\n",
    "        rephrase_question=False,\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "def get_response(question):\n",
    "    chat_history = \"\"\n",
    "    chain = make_chain()\n",
    "    response = chain({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Website into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_docs(\"https://pypi.org/project/chromadb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 58\n",
      "Content:\n",
      " Skip to main content [ .us ](/ref=nav_logo) [ Delivering to Lebanon 66952  Update location  ]() Books __ Select the department you want to search in All Departments Alexa Skills Amazon Devices Amazon One Medical Amazon Pharmacy Amazon Resale Appliances Apps & Games Arts, Crafts & Sewing Audible Books & Originals Automotive Parts & Accessories Baby Beauty & Personal Care Books CDs & Vinyl Cell Phones & Accessories Clothing, Shoes & Jewelry Women Men Girls Boys Baby Collectibles & Fine Art Computers Credit and Payment Cards Digital Music Electronics Garden & Outdoor Gift Cards Grocery & Gourmet Food Handmade Health, Household & Baby Care Home & Business Services Home & Kitchen Industrial & Scientific Just for Prime Kindle Store Luggage & Travel Gear Luxury Stores Magazine Subscriptions Movies & TV Musical Instruments Office Products Pet Supplies Premium Beauty Prime Video Smart Home Software Sports & Outdoors Subscribe & Save Subscription Boxes Tools & Home Improvement Toys & Games Under $10 Video Games Search Amazon [ EN ](/customer-preferences/edit?ie=UTF8&preferencesReturnUrl=%2F&ref_=topnav_lang) [ Hello, sign in Account & Lists\n",
      "\n",
      "\n",
      "Metadata: {'description': 'Discover the best books in Amazon Best Sellers. Find the top 100 most popular Amazon books.', 'keywords': '', 'title': 'Amazon Best Sellers: Best Books', 'url': 'https://www.amazon.com/Best-Sellers-Books/zgbs/books/'}\n",
      "\n",
      "\n",
      "Embeddings: Length: 4096 \n",
      "Embedding Vector: [ 0.00786936 -0.0153204   0.0081309  ...  0.00309851 -0.00513294\n",
      "  0.00351374]\n"
     ]
    }
   ],
   "source": [
    "vector_store = get_chroma_client()\n",
    "lis = vector_store.get(include=['embeddings','metadatas','documents'])\n",
    "print(\"Number of documents:\",len(lis['documents']))\n",
    "print(\"Content:\\n\",lis['documents'][0])\n",
    "print(\"\\n\\nMetadata:\", lis['metadatas'][0])\n",
    "print(\"\\n\\nEmbeddings:\", \"Length:\", len(lis['embeddings'][1]),\"\\nEmbedding Vector:\",lis['embeddings'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Unfortunately, based on the provided context, I'm unable to determine what this website is specifically about. The snippet appears to be a fragment of Amazon's book page, listing various books with their titles, authors, ratings, and prices. However, without more information or access to the full website content, it's challenging to provide a comprehensive answer.\n",
      "\n",
      "If you could provide more context or access to the full website content, I'd be happy to try and assist you further.\n"
     ]
    }
   ],
   "source": [
    "response = get_response(\"What is this website all about\")\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      " [\n",
      "    {\n",
      "        \"title\": \"Amazon Best Sellers: Best Books\",\n",
      "        \"link\": \"https://www.amazon.com/Best-Sellers-Books/zgbs/books\",\n",
      "        \"snippet\": \"64 offers from $2.45. #6. Hello, Baby Animals: A Black-and-White Board Book for Babies That Helps Visual Development (High-Contrast Books) duopress labs. 6,781. Board book. 88 offers from $3.19. #7. I Love You Like No Otter: A Funny and Sweet Animal Board Book for Babies and Toddlers this Christmas (Punderland)\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Most Read Fiction | Amazon Charts\",\n",
      "        \"link\": \"https://www.amazon.com/charts\",\n",
      "        \"snippet\": \"Week of October 6, 2024. The Top 20 Most Sold & Most Read Books of the Week. charts rank books according to the number of copies sold and pre-ordered through Amazon.com, Audible.com, Amazon Books stores, and books read through digital subscription programs (once a customer has read a certain percentage - roughly the length of a free reading ...\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Amazon Best Sellers: Best Books\",\n",
      "        \"link\": \"https://www.amazon.com/Best-Sellers-Books/zgbs/books/283155\",\n",
      "        \"snippet\": \"62 offers from $2.45. #8. The Anxious Generation: How the Great Rewiring of Childhood Is Causing an Epidemic of Mental Illness. Jonathan Haidt. 3,634. Hardcover. 70 offers from $15.24. #9. Spooky Cutie: Coloring Book for Adults and Teens Featuring Adorable Creepy Creatures in Cozy Hygge Moments for Relaxation (Cozy Spaces Coloring)\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Amazon Best Sellers: Best Books\",\n",
      "        \"link\": \"https://www.amzn.com/Best-Sellers-Books/zgbs/books\",\n",
      "        \"snippet\": \"46 offers from $10.50. #6. Into the Uncut Grass. Trevor Noah. 8. Hardcover. 42 offers from $18.20. #7. Spooky Cutie: Coloring Book for Adults and Teens Featuring Adorable Creepy Creatures in Cozy Hygge Moments for Relaxation (Cozy Spaces Coloring)\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"20 Best Books of 2023 So Far, According to Amazon Editors - About Amazon\",\n",
      "        \"link\": \"https://www.aboutamazon.com/news/books-and-authors/amazon-best-books-2023\",\n",
      "        \"snippet\": \"In addition to the overall top 20 Best Books of the Year So Far, the Amazon Books Editorial Team also put together the top 20 picks in popular categories like biography and memoir, literature and fiction, history, mystery and thriller, romance, cookbooks, and children's books (by age)\\u2014making it the perfect list to discover your next favorite read.\"\n",
      "    }\n",
      "]\n",
      "Processed 5 links and stored their content in the vector database.\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "import json\n",
    "\n",
    "def perform_web_search(query, num_results=5):\n",
    "    search_results = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, backend=\"lite\", max_results=num_results):\n",
    "            search_results.append({\n",
    "                \"title\": r[\"title\"],\n",
    "                \"link\": r[\"href\"],\n",
    "                \"snippet\": r[\"body\"]\n",
    "            })\n",
    "    return search_results\n",
    "\n",
    "def get_search_results(query, num_results=5):\n",
    "    results = perform_web_search(query, num_results)\n",
    "    return json.dumps(results, indent=4)\n",
    "\n",
    "def process_and_store_results(query, num_results=5):\n",
    "    search_results = perform_web_search(query, num_results)\n",
    "    \n",
    "    for result in search_results:\n",
    "        url = result['link']\n",
    "        store_docs(url)\n",
    "    \n",
    "    return f\"Processed {len(search_results)} links and stored their content in the vector database.\"\n",
    "\n",
    "# Example usage\n",
    "query = \"ttop 10 selling books in amazon\"\n",
    "search_results_json = get_search_results(query)\n",
    "print(\"Search Results:\\n\", search_results_json)\n",
    "\n",
    "# Process and store the content of the links returned by the search\n",
    "process_message = process_and_store_results(query)\n",
    "print(process_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: However, Amazon's bestseller list changes frequently, and it's not publicly available to access the exact numbers. But I can give you a snapshot of the current top-selling books on Amazon based on various categories. Keep in mind that these rankings are subject to change.\n",
      "\n",
      "Here are the top 10 selling books across multiple categories on Amazon:\n",
      "\n",
      "**Note:** The sales figures are approximate and sourced from Amazon's bestseller list, which is updated hourly.\n",
      "\n",
      "1. **\"The Nightingale\" by Kristin Hannah**: Historical Fiction\n",
      "\t* Average customer review: 4.7/5 stars (over 22,000 reviews)\n",
      "2. **\"The Return\" by Nicholas Sparks**: Romance\n",
      "\t* Average customer review: 4.6/5 stars (over 12,000 reviews)\n",
      "3. **\"It Ends with Us\" by Colleen Hoover**: Romance\n",
      "\t* Average customer review: 4.7/5 stars (over 10,000 reviews)\n",
      "4. **\"Where the Crawdads Sing\" by Delia Owens**: Fiction\n",
      "\t* Average customer review: 4.6/5 stars (over 9,000 reviews)\n",
      "5. **\"Bridgerton: The Duke and I\" by Julia Quinn**: Historical Romance\n",
      "\t* Average customer review: 4.7/5 stars (over 8,000 reviews)\n",
      "6. **\"The Last Thing He Told Me\" by Laura Dave**: Mystery & Thriller\n",
      "\t* Average customer review: 4.5/5 stars (over 7,000 reviews)\n",
      "7. **\"Verity\" by Colleen Hoover**: Psychological Thriller\n",
      "\t* Average customer review: 4.6/5 stars (over 6,000 reviews)\n",
      "8. **\"Apples Never Fall\" by Liane Moriarty**: Fiction\n",
      "\t* Average customer review: 4.5/5 stars (over 5,500 reviews)\n",
      "9. **\"The Family Upstairs\" by Lisa Jewell**: Mystery & Thriller\n",
      "\t* Average customer review: 4.6/5 stars (over 5,000 reviews)\n",
      "10. **\"The Four Winds\" by Kristin Hannah**: Historical Fiction\n",
      "\t* Average customer review: 4.7/5 stars (over 4,500 reviews)\n",
      "\n",
      "Please note that these rankings and sales figures are subject to change and might not reflect the current top-selling books on Amazon.\n",
      "\n",
      "Would you like me to suggest a specific book based on your interests?\n"
     ]
    }
   ],
   "source": [
    "response = get_response(\"hey , can you tell me the top 10 selling books in amazon\")\n",
    "print(\"Answer:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
