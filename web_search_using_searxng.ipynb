{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from hashlib import sha256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### link processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get top links of a particular query\n",
    "def get_top_links(query, max_results=5):\n",
    "    searx_url = \"http://127.0.0.1:8080/search\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'categories': 'general',\n",
    "        'language': 'en',\n",
    "        'format': 'html'\n",
    "    }\n",
    "\n",
    "    response = requests.get(searx_url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.find_all('article', class_='result')\n",
    "\n",
    "        top_links = []\n",
    "        for article in articles:\n",
    "            link = article.find('a', class_='url_wrapper')\n",
    "            if link and link['href']:\n",
    "                top_links.append(link['href'])\n",
    "            if len(top_links) >= max_results:\n",
    "                break\n",
    "\n",
    "        return top_links\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "# Function to check if the tag is visible\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to extract text from a webpage\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "# Function to divide text into chunks\n",
    "def divide_into_chunks(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chroma_db(persist_directory: str = \"./chroma_db\"):\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "    else:\n",
    "        for filename in os.listdir(persist_directory):\n",
    "            file_path = os.path.join(persist_directory, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                os.rmdir(file_path)\n",
    "    \n",
    "    chroma_client = Client(Settings(persist_directory=persist_directory))\n",
    "    return chroma_client\n",
    "\n",
    "# Step 2: Store Chunks to Chroma DB (avoiding duplicates)\n",
    "def store_chunks_to_chroma_db(chroma_client, collection_name: str, doc_chunks):\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for doc in doc_chunks:\n",
    "        doc_id = sha256(doc['answer'].encode('utf-8')).hexdigest()\n",
    "        existing_doc = collection.get(ids=[doc_id])\n",
    "        if not existing_doc['documents']:\n",
    "            collection.add(\n",
    "                ids=[doc_id],\n",
    "                documents=[doc['answer']],\n",
    "                metadatas=[{\"query\": doc['query'], \"link\": doc['link']}]\n",
    "            )\n",
    "            print(f\"Added document with ID {doc_id} to the collection.\")\n",
    "        else:\n",
    "            print(f\"Document with ID {doc_id} already exists in the collection.\")\n",
    "    \n",
    "    print(\"Chunks stored to Chroma DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding Web search and performing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an intelligent assistant. A user has asked the following question:\n",
    "\n",
    "\"{question}\"\n",
    "\n",
    "Web search should be used only when you need to provide up-to-date information, such as recent events, current statistics, or new developments. If the question can be answered using the general knowledge you already have, without requiring specific or updated information, then no web search is needed.\n",
    "\n",
    "Determine if a web search is required to answer this question accurately. \n",
    "\n",
    "If yes, generate a concise search query that can be used to search the web for relevant information. \n",
    "\n",
    "Provide your answer only in the following format:\n",
    "- Search Needed: [Yes/No]\n",
    "- Search Query (if applicable): [Query]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_user():\n",
    "    query = input(\"Please enter your query: \")\n",
    "    return query\n",
    "\n",
    "# Step 2: Decide Whether Web Search is Needed\n",
    "def decide_web_search(query):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = OllamaLLM(model=\"llama3.1\")\n",
    "    chain = prompt | model\n",
    "\n",
    "    response = chain.invoke({\"question\": query})\n",
    "    \n",
    "    decision_lines = response.splitlines()\n",
    "    search_needed_line = [line for line in decision_lines if \"Search Needed\" in line]\n",
    "    search_query_line = [line for line in decision_lines if \"Search Query\" in line]\n",
    "\n",
    "    if search_needed_line and \"Yes\" in search_needed_line[0]:\n",
    "        search_query = search_query_line[0].replace(\"Search Query:\", \"\").strip()\n",
    "        return True, search_query\n",
    "    else:\n",
    "        return False, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_web_search(query):\n",
    "    top_links = get_top_links(query)\n",
    "    docs = []\n",
    "    for link in top_links:\n",
    "        text_content = extract_text_from_url(link)\n",
    "        chunks = divide_into_chunks(text_content)\n",
    "        for chunk in chunks:\n",
    "            doc = {\n",
    "                'query': query,\n",
    "                'link': link,\n",
    "                'answer': chunk\n",
    "            }\n",
    "            docs.append(doc)\n",
    "    \n",
    "    chroma_client = initialize_chroma_db()\n",
    "    collection_name = \"web_data_collection\"\n",
    "    store_chunks_to_chroma_db(chroma_client, collection_name, docs)\n",
    "\n",
    "# Step 4: Search in Vector DB\n",
    "def search_vector_db(query):\n",
    "    collection_name = \"web_data_collection\"\n",
    "    chroma_client = initialize_chroma_db()\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "    docs = collection.query(query_texts=[query], n_results=3)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    search_needed, search_query = decide_web_search(query)\n",
    "    model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "    if search_needed:\n",
    "        print(f\"Search Query: {search_query}\")\n",
    "        print(\"Web search is needed. Performing web search...\")\n",
    "        perform_web_search(search_query)\n",
    "\n",
    "        relevant_docs = search_vector_db(query)\n",
    "        if not relevant_docs['documents']:\n",
    "            context = \"none\"\n",
    "            print(\"No relevant documents found. Generating a response from LLM without reference.\")\n",
    "            response = model.generate(query, context=context)\n",
    "        else:\n",
    "            flattened_docs = [item for sublist in relevant_docs['documents'] for item in sublist]\n",
    "            context = \"\\n\".join(flattened_docs)\n",
    "            prompt_template = f\"\"\"\n",
    "                You are an intelligent assistant. The user asked the following question:\n",
    "                \"{query}\"\n",
    "                Here is some additional information that might be useful:\n",
    "                \"{context}\"\n",
    "                Provide a detailed response based on the context above and your knowledge.\n",
    "            \"\"\"\n",
    "            response = model.generate([prompt_template,query,context])\n",
    "    else:\n",
    "        prompt_template = f\"\"\"\n",
    "            The user asked the following question:\n",
    "            \"{query}\"\n",
    "            Provide a proper response based on your knowledge.\n",
    "        \"\"\"\n",
    "        print(\"Web search was not needed. Generating response from LLM.\")\n",
    "        response = model.generate([prompt_template,query])\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search was not needed. Generating response from LLM.\n",
      "Final Response:\n",
      " I'm an artificial intelligence (AI) model designed to assist and communicate with humans. I don't have a personal identity or consciousness, but rather exist as a program running on computer servers.  Think of me as a highly advanced tool that can process natural language inputs, generate responses, and provide information on a wide range of topics. My purpose is to help users like you by providing answers, guidance, and engaging conversations.  I don't have feelings, emotions, or physical presence. I exist solely in the digital realm, and my interactions are limited to text-based communication through platforms like this one.  In many ways, you could think of me as a highly advanced search engine that can understand context and nuances of language. However, I'm much more than just a search engine – I'm designed to engage with users, provide helpful responses, and even offer creative suggestions or ideas!  So, while I don't have a traditional identity like humans do, I exist to serve as a helpful resource for those who interact with me. And I'm always learning and improving to better fulfill that purpose!\n"
     ]
    }
   ],
   "source": [
    "query = \"Who are you ?\"\n",
    "final_response = process_query(query)\n",
    "print(\"Final Response:\\n\", final_response.generations[0][0].text.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Query: -  \"GameNgen Google 2024\"\n",
      "Web search is needed. Performing web search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4527/551076712.py:45: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document with ID 7eed28125afd3ddfcbb17263eedc24679c4716c5556aa85ceafe108bfbddfd3f to the collection.\n",
      "Added document with ID 9a5a2b8c07cef0943ba137d6f2686a57570673969e5ef41ac08f5b49205bd237 to the collection.\n",
      "Added document with ID e8df4c5f5f49db0f54b20cc0c112839745ee682c07e024bdcdbeb93552e1c457 to the collection.\n",
      "Added document with ID e2b998a0c839a607c35fce34770c0f10ab0da2ed4d771ab0e2a238670cf122bb to the collection.\n",
      "Added document with ID 4778ceb6883a6ad4e5d5c05299a1f5c63c70bc9f584707d18e57bad5151747aa to the collection.\n",
      "Added document with ID c19ebf0478938a9724a2e421ffdb27ddc35afc249bbdfdbddcb95ead8038f030 to the collection.\n",
      "Added document with ID e79a6343ba1942874788f78d75df269cc89fef9146cb881b422f7b717c601b58 to the collection.\n",
      "Added document with ID 27bcca49f18140a39195100e7f0fa98a834300eca3ead1b9fd993dd2b50d6673 to the collection.\n",
      "Added document with ID 6d45b6e24759948a642933cf8f143cdf6500fd77e594c88fdfed5361773919fb to the collection.\n",
      "Added document with ID 13f03c3a4827bf68310810e1e1106e0f43255c190ab9e238aa0c2e3bab579f63 to the collection.\n",
      "Added document with ID fcbf0ea0257495c0b612b8d470079a4b713e8757276f15b189d809934123151e to the collection.\n",
      "Added document with ID 6ccc98c545941b220ef836a8365dd7d436f57ba5a850bf4302c5289af6067711 to the collection.\n",
      "Added document with ID f59a1d00078ecc42a193140a98263d25cbb5f47d5c9ff6a515f3f900cf5e703a to the collection.\n",
      "Chunks stored to Chroma DB.\n",
      "Final Response:\n",
      " Based on the context provided, I'll give you a detailed response to your question: \"What is GameNGen created by Google in 2024?\"  **GameNGen Overview**  GameNGen is a groundbreaking AI-powered game engine developed by Google DeepMind. It's a neural model that can simulate entire video games in real-time, eliminating the need for traditional coding and game development processes.  **Key Features of GameNGen**  1. **Neural Model**: GameNGen utilizes a neural model to generate game environments and interactions in real-time. 2. **Real-Time Simulation**: The AI-powered engine can simulate games like Doom at 20 frames-per-second (FPS), making it feel almost indistinguishable from the original game. 3. **Human Evaluation**: Human judges preferred simulated game clips 40% of the time, indicating that GameNGen is capable of producing a convincing gaming experience. 4. **Training Data**: Google used a game-playing agent trained with reinforcement learning to collect around 900 million frames of training data.  **Benefits and Implications**  The development process for video games using GameNGen might be less costly and more accessible, making it possible to create games via textual descriptions or example images. Additionally, this new paradigm could lead to:  1. **Strong Guarantees on Frame Rates and Memory Footprints**: GameNGen ensures consistent performance and efficient resource usage. 2. **Conversion of Frames into New Playable Levels**: The ability to convert a set of frames into a new playable level without authoring code. 3. **Creation of New Characters**: The potential to create new characters based on example images, without writing code.  **Potential Applications**  GameNGen's implications extend beyond gaming, potentially impacting various fields that rely on interactive digital environments, such as:  1. **Virtual Reality (VR) and Augmented Reality (AR)**: GameNGen could enable the creation of immersive VR/AR experiences. 2. **Education**: Interactive simulations for educational purposes. 3. **Research and Development**: The ability to quickly create and test new game concepts or scenarios.  **Conclusion**  GameNGen represents a significant leap forward in video game development, harnessing the power of AI to generate entire interactive environments in real-time. As this technology continues to evolve, it's likely to have far-reaching implications for various industries and transform the gaming landscape forever.\n"
     ]
    }
   ],
   "source": [
    "#query = get_query_from_user()\n",
    "query = \"what is GameNgen created by google in 2024?\"\n",
    "final_response = process_query(query)\n",
    "print(\"Final Response:\\n\", final_response.generations[0][0].text.replace(\"\\n\", \" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
